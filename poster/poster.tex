\documentclass[a0,landscape]{a0poster}
\input{preamble.tex} % after the basics
\definecolor{PMS310C}{RGB}{62,192,197} % Official MTU colorway

\begin{document}
\begin{tikzpicture}[remember picture,overlay]
  \node[anchor=north east, xshift=-5mm, yshift=-5mm] 
    at (current page.north east) {\includegraphics[height=7cm]{logo.png}};
\end{tikzpicture}

%----------------------------------------------------------------------------------------
% HEADER
%----------------------------------------------------------------------------------------
\vspace*{-2cm} % reduce magnitude if anything touches the top edge

{\setlength{\tabcolsep}{0pt}%
\renewcommand{\arraystretch}{1.05}%

\noindent
\begin{minipage}[t]{0.33\textwidth}
  {\Huge\bfseries\color{PMS310C} Fast Julia}\\[1ex]
  {\Large\itshape Optimizing Dense ODE Systems}\\[2ex]
  {\large Daniel Henderson}\\
  {\normalsize Michigan Technological University}\\
  {\small\ttfamily dphender@mtu.edu}
\end{minipage}%
\begin{minipage}[t]{0.40\textwidth}\vspace{0pt}
{\Large\textbf{Abstract}}
{\normalsize
We benchmark naïve and optimized Julia kernal implementations of the right-hand-side of $\vx'(t) = \vf(\vx,\vp,t)$, using the 3D Rössler system as a case study.
Benchmarks show that tuned in-place or static-array variants cut runtime by $\approx4$--$5\times$ and reduce allocations from $\sim6.4\times 10^5$ per solve to a few dozen compared to the naïve baseline.}
\end{minipage}
}

\vspace{0.20cm}
\hrulefill
\vspace{0.20cm}

%----------------------------------------------------------------------------------------
% BEGIN: BODY (3 COLUMNS)
%----------------------------------------------------------------------------------------
\begin{paracol}{3}
  \setcolumnwidth{0.28\textwidth,0.44\textwidth,0.28\textwidth}

%----------------------------------------------------------------------------------------
% BEGIN: LEFT COLUMN
%----------------------------------------------------------------------------------------
\section*{Introduction}

\noindent
ODE workflows often involve ensembles: many solves across parameters, initial conditions,
or long time horizons. In that regime, per-call overhead in the ODE right-hand side
(RHS) function---especially heap allocations---can dominate runtime and make otherwise
simple experiments impractically slow.
We explore techniques to accelerate classical Runge-Kutta (RK4) integration on the Rössler ODE system
\begin{flalign}
  \tag{Rössler} \label{eq:rossler}
  \quad \vx'(t) = \vf(\vx,\, \vp, \, t) = 
  \vect{
    -x_2 \,-\, x_3 \\
    x_1 \,+\, a x_2 \\
    b \,+\, x_3 (x_1 \,-\, c) \\
  }
    ~ : ~ \begin{cases*}
    \vx = (x_1, \, x_2, \, x_3)^\top \text{ (state variables)}\\
    \vp = (a, \, b, \, c)^\top \text{ (parameters)} 
  \end{cases*}, &&
\end{flalign}
where the canonical parameters are $a=0.2$, $b=0.2$, $c=5.7$.
During benchmarking, we fix initial condition $\vx_0=(1,1,1)$ and use the canonical parameters above.
All experiments integrate~\eqref{eq:rossler} over $t \in [0,T]$ using classical fixed-step RK4.
Experiments demonstrate C-like performance is achievable using \texttt{DifferentialEquations.jl}~\cite{DifferentialEquations.jl-2017}
from the SciML ecosystem, following documented techniques for optimizing the model RHS $\vf$
\cite{SciMLTutorial,DiffEqDocsFasterODEExample}.

\medskip

\subsection*{Computational Cost}
RK4 advances $\vx_n \approx \vx(t_n) \text{ via } \vx_{n+1} = \vx_n + \frac{h}{6} (k_1 + 2k_2 + 2k_3 + k_4)$.
The variable cost at stages $k_1,\ldots,k_4$ is evaluating $\vf$ at intermediate points;
which depend on the previous stage. Thus, the total flop count for final simulation time $T$ and step size $h$
is $O(N)$, where $N=T/h$ is the total number of steps. Therefore, halving the step size (thereby doubling $N$)
doubles the number of flops and should approximately double runtime, assuming overhead remains negligible
and the CPU scales linearly. The linear scaling of RK4 is a key expectation verified in our benchmarks.

\medskip


\subsection*{Heap vs. Stack Allocation} 
In Julia, the ``allocations'' reported by \texttt{@time} and \texttt{BenchmarkTools.jl} are heap allocations, 
which are costly because they introduce pointer indirection and add work for the garbage collector (GC). By
contrast, fixed-size temporaries whose layout is known at compile time can usually be stored inline and
 kept off the heap (in a stack frame or registers), making their creation essentially free. Here, 
 ``stack allocation'' is used loosely to mean ``no heap allocation''—Julia does not directly report 
 stack usage, so measuring it typically requires external profilers (e.g., \texttt{perf}, Instruments)
 or inspection of generated code. Regular \texttt{Array}s must be heap allocated since their size is 
 decided at runtime, while tuples, small immutable \texttt{struct}s, and 
 \texttt{StaticArrays.jl} (\texttt{SVector}) can often avoid heap allocation entirely. Our benchmarks 
 show how in-place updates and static-sized states remove per-call heap allocations in the Rössler RHS.

\medskip

%------------------------ Bottom Column Visual  ------------------------
\begin{mdframed}[backgroundcolor=gray!10]
\textbf{Experiment 1 setup.} Classical RK4 with step size $h=0.01$ for $T=200$, so $n_\text{steps}=T/h=20{,}000$. IC $(1,1,1)$, parameters $(0.2,0.2,5.7)$. Variants: naïve allocating right-hand side (RHS), tuned out-of-place, naïve/tuned in-place, \texttt{SVector} naïve/tuned, type-stable, and automatic-differentiation (AD)-ready.

\medskip
\textbf{Metrics.} Benchmarks use \texttt{BenchmarkTools.jl}; we report median wall-clock time and heap allocations for (a) a single RHS call and (b) a full RK4 solve. Allocation-free entries indicate no heap allocation (stack/register temporaries).

\medskip
\textbf{Why it matters.} Ensembles and long horizons magnify per-call overhead. Tightening the RHS kernel pays dividends across every solver step and parameter sweep.
\end{mdframed}


\section*{Experiment 1: Optimizing the Rössler ODE System}

\noindent
We compare out-of-place and in-place Julia implementations
of the \eqref{eq:rossler} system, together with a type-stable fixed/static-size variant:
\begin{itemize}
  \item naïve out-of-place code allocates a new array at each right-hand-side call, triggering garbage collection;
  \item optimized in-place code eliminates per-call allocations for \texttt{Vector}-based states; and
  \item a \texttt{StaticArrays.jl}/\texttt{SVector} implementation keeps state off the heap, often fastest for very small systems.
\end{itemize}

\medskip
\hrulefill
\medskip

\switchcolumn

%----------------------------------------------------------------------------------------
% BEGIN: MIDDLE COLUMN
%----------------------------------------------------------------------------------------
\noindent\textbf{Performance ladder highlights.}
\begin{itemize}
  \item \texttt{StaticArrays} keep the three-state RHS off the heap, reducing GC noise.
  \item Fixed-size math improves cache locality and throughput for tight ODE kernels.
  \item \texttt{@inbounds} removes bounds checks once dimensions are known at compile time.
  \item \texttt{@inline} encourages the compiler to fuse the RHS into the solver loop, trimming call overhead.
  \item Fewer checks and calls leave the CPU focused on arithmetic in the hot loop.
\end{itemize}

\medskip

\noindent
We optimize each implementation using macros \texttt{@inbounds}, \texttt{@inline}, and we see
the lowered code to understand the impact of these changes.


\subsection*{Results and Analysis}

% --- Figures: speedup plot ---
\noindent
\begin{minipage}{\linewidth}
\centering
\begin{minipage}[t]{0.475\linewidth} % Left figure
  \includegraphics[width=0.95\linewidth]{rk4_fixed_speedup_solve.png}
  {\captionsetup{hypcap=false}%
  \captionof{figure}{Experiment 1 speedup for fixed-step RK4 solves relative to the naïve out-of-place baseline `rossler\_naive`, using $\text{speedup}=\mathrm{median\_time}(\texttt{rossler\_naive})/\mathrm{median\_time}(\text{variant})$.}}
\end{minipage}
\hfill
\begin{minipage}[t]{0.475\linewidth} % Right figure
  \includegraphics[width=0.95\linewidth]{rk4_fixed_allocs_solve.png}
  {\captionsetup{hypcap=false}%
  \captionof{figure}{Allocations per RK4 solve. In-place and \texttt{SVector} variants eliminate heap traffic, enabling predictable runtimes across long integrations.}}
\end{minipage}

\medskip
\end{minipage}

\begin{table}[ht]
\centering
\resizebox{0.8\linewidth}{!}{\input{tables/rk4_fixed_rhs_table.tex}}
\caption{Per-call Rössler RHS benchmark medians (ns) and heap allocations across implementation variants.}
\end{table}

\vspace{1em}

\begin{table}[ht]
\centering
\resizebox{0.8\linewidth}{!}{\input{tables/rk4_fixed_solve_table.tex}}
\caption{Fixed-step RK4 solve benchmarks for $t \in [0,T]$ with step size $h$; heap allocations reported per solve.}
\end{table}

\medskip

\noindent\textbf{Key observations.}
\begin{itemize}
  \item In-place and \texttt{SVector} RHS variants eliminate per-call heap allocations; solve-level allocations drop from $\approx6.4\times 10^5$ to a few dozen, reducing GC overhead.
  \item Removing RHS allocations yields multi-$\times$ speedups for full RK4 solves at fixed $h$, since ensembles and long horizons amplify per-call overhead.
  \item Fastest full solves come from the tuned \texttt{SVector} variant, closely followed by the AD-ready implementation that keeps allocations negligible.
\end{itemize}

\medskip
\hrulefill
\medskip

\switchcolumn
%----------------------------------------------------------------------------------------
% BEGIN: THIRD COLUMN
%----------------------------------------------------------------------------------------
%------------------------ Conclusion ------------------------
\section*{Optimization ladder for Rössler RHS variants (L0–L5).}

\medskip
\hrulefill
\medskip

\begin{minipage}[c]{0.45\linewidth}
  \centering
  \includegraphics[width=\linewidth]{optimizations.png}
\end{minipage}

\medskip
\hrulefill 
\medskip 

%------------------------ Conclusion ------------------------
\section*{Conclusion: Optimizing ODE systems in Julia}
\noindent
Experiment 1 confirms that tightening the RHS kernel delivers most of the speedup: moving from naïve allocation to tuned in-place or \texttt{SVector} code removes GC noise, shrinks solve time by $\sim5\times$, and keeps the solver stable across long trajectories.

\medskip


\textbf{Practical tips for fast RHS code.}
\begin{itemize}
  \item Write in-place updates for solver-friendly mutability; reserve allocations for setup, not per-step work.
  \item Use \texttt{SVector} for very small systems (roughly $n \lesssim 16$--$32$ states) to keep data off the heap; fall back to \texttt{Vector} for larger dimensions to avoid compile-time bloat.
  \item Validate correctness, then add \texttt{@inbounds} and \texttt{@inline} to strip bounds checks and call overhead; keep arguments type-stable so the compiler can inline aggressively.
\end{itemize}


\medskip

\noindent\textbf{Outlook.} The same kernel work carries to stiff problems and differentiation workflows; adding implicit integrators or adjoints next should reuse these allocation-free RHS variants without sacrificing performance.


\noindent
%------------------------ REFERENCES ------------------------
\nocite{*}
\bibliographystyle{plain}
\bibliography{mybib}

\end{paracol}
\end{document}
