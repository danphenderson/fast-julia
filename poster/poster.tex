\documentclass[a0,landscape]{a0poster}
\input{preamble.tex} % after the basics
\definecolor{PMS310C}{RGB}{62,192,197} % Official MTU colorway

\begin{document}
\begin{tikzpicture}[remember picture,overlay]
  \node[anchor=north east, xshift=-5mm, yshift=-5mm] 
    at (current page.north east) {\includegraphics[height=7cm]{logo.png}};
\end{tikzpicture}

%----------------------------------------------------------------------------------------
% HEADER
%----------------------------------------------------------------------------------------
\vspace*{-2cm} % reduce magnitude if anything touches the top edge

{\setlength{\tabcolsep}{0pt}%
\renewcommand{\arraystretch}{1.05}%

\noindent
\begin{minipage}[t]{0.34\textwidth}
  {\Huge\bfseries\color{PMS310C} Fast Julia}\\[1ex]
  {\Large\itshape Optimizing Dense Systems of ODEs}\\[2ex]
  {\large Daniel Henderson}\\
  {\normalsize Michigan Technological University}\\
  {\small\ttfamily dphender@mtu.edu}
\end{minipage}%
\begin{minipage}[t]{0.24\textwidth}\vspace{0pt}
{\Large\textbf{Abstract}}
{\normalsize
We compare naïve and optimized Julia implementations for solving
small to moderate systems of (ODEs), $\vx'(t) = \vf(\vx, t)$ with
$\dim(\vx) \leq 10^2$. We demonstrate how to asses and resolve performance 
bottlenecks using the Rössler system as a case study. Numerical Experiments
show that a sequence of targeted changes—in-place updates, type stability—can
yield C-like performance while preserving code clarity. Benchmarks for our single experiment (fixed-step RK4 on the Rössler system) show that tuned in-place or static-array variants cut runtime by $\approx4\times$ and eliminate hundreds of thousands of allocations compared to the naïve baseline.}
\end{minipage}
}

\vspace{0.20cm}
\hrulefill
\vspace{0.20cm}

%----------------------------------------------------------------------------------------
% BEGIN: BODY (3 COLUMNS)
%----------------------------------------------------------------------------------------
\begin{paracol}{3}
  \setcolumnwidth{0.28\textwidth,0.44\textwidth,0.28\textwidth}

%----------------------------------------------------------------------------------------
% BEGIN: LEFT COLUMN
%----------------------------------------------------------------------------------------
\section*{Introduction}

\noindent
\begin{center}
  \includegraphics[width=0.9\linewidth]{pipeline_placeholder.pdf}
  {\captionsetup{hypcap=false}%
  \captionof{figure}{Experiment 1 pipeline: define Rössler RHS variants, integrate with fixed-step RK4, and benchmark runtime/allocations for both the RHS call and full solve.}}
\end{center}

\medskip
\hrulefill
\medskip

\noindent
ODE workflows often involve ensembles: many solves across parameters, initial conditions,
or long time horizons. In that regime, per-call overhead to the ODE model function
—especially allocations—dominates and can make otherwise simple experiments impractically slow.

\medskip

\noindent
The Rössler ODE system is defined as
\begin{flalign*}
  \tag{Rössler} \label{eq:rossler}
  \quad \vx'(t) = \vf(\vx,\, \vp, \, t) = 
  \vect{
    -x_2 \,-\, x_3 \\
    x_1 \,+\, a x_2 \\
    b \,+\, x_3 (x_1 \,-\, c) \\
  }
    ~ : ~ \begin{cases*}
    \vx = (x_1, \, x_2, \, x_3)^\top \text{ (state variables)}\\
    \vp = (a, \, b, \, c)^\top \text{ (parameters)} 
  \end{cases*}, &&
\end{flalign*}

\noindent
where canonical parameters are $a=0.2$, $b=0.2$, $c=5.7$.
The \ref{eq:rossler} system is non-stiff and exhibits chaotic dynamics, making it suitable
benchmark problem for ODE solvers. Numerical integration is done with the classical 4th-order Runge-Kutta (RK4) method - an explicit
solver with $4$-stages, leading to 4 evaluations of $\vf$ at each fixed step. We seek $\vx(t)$ for $t \in [0, T]$ subject
to IC $\vx_0=(1, 1, 1)$ and parameters $\vp=(0.1, 0.1, 15.7)$.

\medskip

\noindent
The goal is to achieve the fastest possible integration time for this system, while mainting code clarity.
C-like performance is achieved using \texttt{DifferentialEquations.jl}~\cite{DifferentialEquations.jl-2017}
from the SciML ecosystem, utilizing documented techniques for optimizing the model code $\vf$
[\cite{SciMLTutorial}, \cite{DiffEqDocsFasterODEExample}].

\medskip

\subsection*{Computational Cost:} RK4 advances $\vx_n \approx \vx(t_n) \text{ via } \vx_{n+1} = \vx_n + \frac{h}{6} (k_1 + 2k_2 + 2k_3 + k_4)$.
The variable cost at stages $k_1,\ldots,k_4$ is evaluating $\vf$ at intermediate points;
which depend on the previous stage. Thus, the total flop count for final simulation time $T$ and step size $h$
is $O(N)$, where $N=T/h$ is the total number of steps. Therefore, halving the step size (thereby doubling $N$)
doubles the number of flops and should approximately double runtime, assuming overhead remains negligible
and the CPU scales linearly. The linear scaling of RK4 is a key expectation verified in our benchmarks.

\medskip

\subsection*{Heap vs. Stack Allocation:} Heap vs. Stack Allocation: In Julia (and low-level languages like C),
data can often be allocated on the stack instead of the heap. Stack allocation is fast and has essentially no
overhead for creation or cleanup – memory is managed by the function call stack and automatically reclaimed 
when out of scope. Our benchmarks will illustrate how different coding patterns for the Rössler ODE affect 
whether memory is allocated on the heap or kept on the stack. In particular, we will see how using in-place
updates and fixed size arrays can eliminate most heap allocations by leveraging stack storage.
\footnote{``Stack allocation`` herein refers to any memory allocation that doesn't occur at runtime.
Measuring a program's stack allocation requires third party external tools.}

\medskip

%------------------------ Bottom Column Visual  ------------------------
\begin{mdframed}[backgroundcolor=gray!10]
\textbf{Experiment 1 setup.} Classical RK4 with $h=0.01$ for $T=200$ ($n_\text{steps}=20{,}000$), IC $(1,1,1)$, parameters $(0.1,0.1,15.7)$. Variants: naïve allocating RHS, tuned out-of-place, naïve/tuned in-place, \texttt{SVector} naïve/tuned, type-stable, and AD-ready.

\medskip
\textbf{Metrics.} Benchmarks use \texttt{BenchmarkTools.jl}; we report median wall-clock time and heap allocations for (a) a single RHS call and (b) a full RK4 solve. Allocation-free entries indicate work stays on the stack.

\medskip
\textbf{Why it matters.} Ensembles and long horizons magnify per-call overhead. Tightening the RHS kernel pays dividends across every solver step and parameter sweep.
\end{mdframed}

\switchcolumn

%----------------------------------------------------------------------------------------
% BEGIN: MIDDLE COLUMN
%----------------------------------------------------------------------------------------
\section*{Experiment 1: Optimizing the Rössler ODE System}

\noindent
We compare out-of-place and in-place Julia implementations
of the \eqref{eq:rossler} system, together with a type-stable fixed/static-size variant:
\begin{itemize}
  \item naïve out-of-place code allocates a new array at each right-hand-side call, triggering garbage collection;
  \item optimized in-place code eliminates per-call allocations for \texttt{Vector}-based states; and
  \item a \texttt{StaticArrays.jl}/\texttt{SVector} implementation enables stack allocation, often fastest for very small systems.
\end{itemize}

\medskip

\noindent\textbf{Performance ladder highlights.}
\begin{itemize}
  \item \texttt{StaticArrays} keep the three-state RHS on the stack, reducing heap pressure and GC noise.
  \item Fixed-size math improves cache locality and throughput for tight ODE kernels.
  \item \texttt{@inbounds} removes bounds checks once dimensions are known at compile time.
  \item \texttt{@inline} encourages the compiler to fuse the RHS into the solver loop, trimming call overhead.
  \item Fewer checks and calls leave the CPU focused on arithmetic in the hot loop.
\end{itemize}

\medskip

\noindent
We optimize each implementation using macros \texttt{@inbounds}, \texttt{@inline}, and we see
the lowered code to understand the impact of these changes.

\medskip
\hrulefill
\medskip

\subsection*{Results and Analysis}

% --- Figures: speedup plot ---
\noindent
\begin{minipage}{\linewidth}
\centering
\begin{minipage}[t]{0.475\linewidth} % Left figure
  \includegraphics[width=0.95\linewidth]{rk4_fixed_speedup_solve_log10.png}
  {\captionsetup{hypcap=false}%
  \captionof{figure}{Experiment 1 speedup for fixed-step RK4 solves relative to the naïve out-of-place baseline `rossler\_naive`, using $\text{speedup}=\mathrm{median\_time}(\texttt{rossler\_naive})/\mathrm{median\_time}(\text{variant})$.}}
\end{minipage}
\hfill
\begin{minipage}[t]{0.475\linewidth} % Right figure
  \includegraphics[width=0.95\linewidth]{rk4_fixed_allocs_solve.png}
  {\captionsetup{hypcap=false}%
  \captionof{figure}{Allocations per RK4 solve. In-place and \texttt{SVector} variants eliminate heap traffic, enabling predictable runtimes across long integrations.}}
\end{minipage}
\end{minipage}
\begin{center}
  \begin{tabular}{lrrrr}
    \toprule
    Variant & Median (ns) & Median (ms) & Allocs & Bytes \\
    \midrule
    naïve (alloc) & 42 & 4.2e-05 & 2 & 80 \\
    oop (tuned) & 42 & 4.2e-05 & 2 & 80 \\
    in-place (naïve) & 41 & 4.1e-05 & 0 & 0 \\
    in-place (tuned) & 20.5 & 2.05e-05 & 0 & 0 \\
    SVector (naïve) & 0.001 & 1e-09 & 0 & 0 \\
    SVector (tuned) & 0.001 & 1e-09 & 0 & 0 \\
    type-stable & 42 & 4.2e-05 & 2 & 80 \\
    AD-ready & 41 & 4.1e-05 & 0 & 0 \\
    \bottomrule
  \end{tabular}
  {\captionsetup{hypcap=false}%
  \captionof{table}{Per-call Rössler RHS benchmark medians and allocations across implementation variants.}}

  \vspace{0.75em}

  \begin{tabular}{lrrrrrr}
    \toprule
    Variant & dt & nsteps & Median (ns) & Median (ms) & Allocs & Bytes \\
    \midrule
    naïve (alloc) & 0.01 & 20000 & 7.41e+06 & 7.41 & 640066 & 25603088 \\
    oop (tuned) & 0.01 & 20000 & 7.31e+06 & 7.31 & 640066 & 25603088 \\
    in-place (naïve) & 0.01 & 20000 & 2.03e+06 & 2.03 & 50 & 2624 \\
    in-place (tuned) & 0.01 & 20000 & 1.88e+06 & 1.88 & 50 & 2624 \\
    SVector (naïve) & 0.01 & 20000 & 1.64e+06 & 1.64 & 22 & 1504 \\
    SVector (tuned) & 0.01 & 20000 & 1.46e+06 & 1.46 & 22 & 1504 \\
    type-stable & 0.01 & 20000 & 8.27e+06 & 8.27 & 640066 & 25603088 \\
    AD-ready & 0.01 & 20000 & 1.55e+06 & 1.55 & 22 & 1504 \\
    \bottomrule
  \end{tabular}
  {\captionsetup{hypcap=false}%
  \captionof{table}{Fixed-step RK4 solve benchmarks for $t \in [0,T]$ with step size $dt$; allocations reported per solve.}}
\end{center}

\medskip

\noindent\textbf{Key observations.}
\begin{itemize}
  \item Tuned in-place RHS cuts per-call time roughly in half versus naïve out-of-place, and the stack-resident \texttt{SVector} versions are effectively allocation-free.
  \item Solve-level allocations drop from $\approx6.4\times 10^5$ to a few dozen (or zero) once the RHS stops allocating, stabilizing runtimes across long horizons.
  \item Fastest full solves come from the tuned \texttt{SVector} variant ($\sim5\times$ faster than baseline), closely followed by the AD-ready implementation that keeps allocations negligible.
\end{itemize}

\medskip
\hrulefill
\medskip

\switchcolumn
%----------------------------------------------------------------------------------------
% BEGIN: THIRD COLUMN
%----------------------------------------------------------------------------------------

%------------------------ Conclusion ------------------------
\section*{Conclusion: Optimizing ODEs in Julia}
\noindent
Experiment 1 confirms that tightening the RHS kernel delivers most of the speedup: moving from naïve allocation to tuned in-place or \texttt{SVector} code removes GC noise, shrinks solve time by $\sim5\times$, and keeps the solver stable across long trajectories.

\medskip

\noindent\textbf{Practical tips for fast RHS code.}
\begin{itemize}
  \item Write in-place updates for solver-friendly mutability; reserve allocations for setup, not per-step work.
  \item Use \texttt{SVector} for very small systems to keep data on the stack; fall back to \texttt{Vector} for larger dimensions to avoid compile-time bloat.
  \item Validate correctness, then add \texttt{@inbounds} and \texttt{@inline} to strip bounds checks and call overhead; keep arguments type-stable so the compiler can inline aggressively.
\end{itemize}

\medskip

\noindent\textbf{Outlook.} The same kernel work carries to stiff problems and differentiation workflows; adding implicit integrators or adjoints next should reuse these allocation-free RHS variants without sacrificing performance.
\medskip

\noindent
%------------------------ REFERENCES ------------------------
\nocite{*}
\bibliographystyle{plain}
\bibliography{mybib}

\end{paracol}
\end{document}
