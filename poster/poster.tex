\documentclass[a0,landscape]{a0poster}
\input{preamble.tex} % after the basics
\definecolor{PMS310C}{RGB}{62,192,197} % Official MTU colorway

\begin{document}
\begin{tikzpicture}[remember picture,overlay]
  \node[anchor=north east, xshift=-5mm, yshift=-5mm] 
    at (current page.north east) {\includegraphics[height=7cm]{logo.png}};
\end{tikzpicture}

%----------------------------------------------------------------------------------------
% HEADER
%----------------------------------------------------------------------------------------
\vspace*{-2cm} % reduce magnitude if anything touches the top edge

{\setlength{\tabcolsep}{0pt}%
\renewcommand{\arraystretch}{1.05}%

\noindent
\begin{minipage}[t]{0.33\textwidth}
  {\Huge\bfseries\color{PMS310C} Fast Julia}\\[1ex]
  {\Large\itshape Optimizing Dense ODE Systems}\\[2ex]
  {\large Daniel Henderson}\\
  {\normalsize Michigan Technological University}\\
  {\small\ttfamily dphender@mtu.edu}
\end{minipage}%
\begin{minipage}[t]{0.40\textwidth}\vspace{0pt}
{\Large\textbf{Abstract}}
{\normalsize
We benchmark naïve and optimized Julia implementations for solving small to moderate systems of ODEs, $\vx'(t) = \vf(\vx,\vp,t)$, focusing on the 3D Rössler system as a case study. We demonstrate how to assess and resolve performance bottlenecks with targeted changes—in-place updates and type-stable code—while preserving clarity. Benchmarks for a fixed-step RK4 solve show that tuned in-place or static-array variants cut runtime by $\approx4$--$5\times$ and reduce allocations from $\sim6.4\times 10^5$ per solve to a few dozen compared to the naïve baseline.}
\end{minipage}
}

\vspace{0.20cm}
\hrulefill
\vspace{0.20cm}

%----------------------------------------------------------------------------------------
% BEGIN: BODY (3 COLUMNS)
%----------------------------------------------------------------------------------------
\begin{paracol}{3}
  \setcolumnwidth{0.28\textwidth,0.44\textwidth,0.28\textwidth}

%----------------------------------------------------------------------------------------
% BEGIN: LEFT COLUMN
%----------------------------------------------------------------------------------------
\section*{Introduction}

\noindent
ODE workflows often involve ensembles: many solves across parameters, initial conditions,
or long time horizons. In that regime, per-call overhead to the ODE model function
—especially allocations—dominates and can make otherwise simple experiments impractically slow.

\medskip

\noindent
The Rössler ODE system is defined as
\begin{flalign*}
  \tag{Rössler} \label{eq:rossler}
  \quad \vx'(t) = \vf(\vx,\, \vp, \, t) = 
  \vect{
    -x_2 \,-\, x_3 \\
    x_1 \,+\, a x_2 \\
    b \,+\, x_3 (x_1 \,-\, c) \\
  }
    ~ : ~ \begin{cases*}
    \vx = (x_1, \, x_2, \, x_3)^\top \text{ (state variables)}\\
    \vp = (a, \, b, \, c)^\top \text{ (parameters)} 
  \end{cases*}, &&
\end{flalign*}

\noindent
where the canonical Rössler parameters are $a=0.2$, $b=0.2$, $c=5.7$.
The system~\eqref{eq:rossler} is non-stiff and exhibits chaotic dynamics, making it a
standard benchmark problem for ODE solvers.
where canonical parameters are $a=0.2$, $b=0.2$, $c=5.7$.

\medskip

\noindent
For benchmarking purposes, we fix $\vx_0=(1,1,1)$ and choose
$\vp=(0.1,\,0.1,\,15.7)$, which yields representative dynamics while avoiding
stiffness. All experiments integrate~\eqref{eq:rossler} over $t \in [0,T]$
using classical fixed-step RK4.

\medskip

\noindent
The goal is to achieve the fastest possible integration time for this system, while maintaining code clarity.
High performance can be achieved using \texttt{DifferentialEquations.jl}~\cite{DifferentialEquations.jl-2017}
from the SciML ecosystem, utilizing documented techniques for optimizing the model code $\vf$
[\cite{SciMLTutorial}, \cite{DiffEqDocsFasterODEExample}].

\medskip

\subsection*{Computational Cost}
RK4 advances $\vx_n \approx \vx(t_n) \text{ via } \vx_{n+1} = \vx_n + \frac{h}{6} (k_1 + 2k_2 + 2k_3 + k_4)$.
The variable cost at stages $k_1,\ldots,k_4$ is evaluating $\vf$ at intermediate points;
which depend on the previous stage. Thus, the total flop count for final simulation time $T$ and step size $h$
is $O(N)$, where $N=T/h$ is the total number of steps. Therefore, halving the step size (thereby doubling $N$)
doubles the number of flops and should approximately double runtime, assuming overhead remains negligible
and the CPU scales linearly. The linear scaling of RK4 is a key expectation verified in our benchmarks.

\medskip


\subsection*{Heap vs. Stack Allocation} 
In Julia, the ``allocations'' reported by \texttt{@time} and \texttt{BenchmarkTools.jl} are heap allocations, 
which are costly because they introduce pointer indirection and add work for the garbage collector. By
contrast, fixed-size temporaries whose layout is known at compile time can usually be stored inline and
 kept off the heap (in a stack frame or registers), making their creation essentially free. Here, 
 ``stack allocation'' is used loosely to mean ``no heap allocation''—Julia does not directly report 
 stack usage, so measuring it typically requires external profilers (e.g., \texttt{perf}, Instruments)
 or inspection of generated code. Regular \texttt{Array}s must be heap allocated since their size is 
 decided at runtime, while tuples, small immutable \texttt{struct}s, and 
 \texttt{StaticArrays.jl} (\texttt{SVector}) can often avoid heap allocation entirely. Our benchmarks 
 show how in-place updates and static-sized states remove per-call heap allocations in the Rössler RHS.

\medskip

%------------------------ Bottom Column Visual  ------------------------
\begin{mdframed}[backgroundcolor=gray!10]
\textbf{Experiment 1 setup.} Classical RK4 with $h=0.01$ for $T=200$ ($n_\text{steps}=20{,}000$), IC $(1,1,1)$, parameters $(0.1,0.1,15.7)$. Variants: naïve allocating RHS, tuned out-of-place, naïve/tuned in-place, \texttt{SVector} naïve/tuned, type-stable, and AD-ready.

\medskip
\textbf{Metrics.} Benchmarks use \texttt{BenchmarkTools.jl}; we report median wall-clock time and heap allocations for (a) a single RHS call and (b) a full RK4 solve. Allocation-free entries indicate no heap allocation (stack/register temporaries).

\medskip
\textbf{Why it matters.} Ensembles and long horizons magnify per-call overhead. Tightening the RHS kernel pays dividends across every solver step and parameter sweep.
\end{mdframed}


\section*{Experiment 1: Optimizing the Rössler ODE System}

\noindent
We compare out-of-place and in-place Julia implementations
of the \eqref{eq:rossler} system, together with a type-stable fixed/static-size variant:
\begin{itemize}
  \item naïve out-of-place code allocates a new array at each right-hand-side call, triggering garbage collection;
  \item optimized in-place code eliminates per-call allocations for \texttt{Vector}-based states; and
  \item a \texttt{StaticArrays.jl}/\texttt{SVector} implementation keeps state off the heap, often fastest for very small systems.
\end{itemize}

\medskip
\hrulefill
\medskip

\switchcolumn

%----------------------------------------------------------------------------------------
% BEGIN: MIDDLE COLUMN
%----------------------------------------------------------------------------------------
\noindent\textbf{Performance ladder highlights.}
\begin{itemize}
  \item \texttt{StaticArrays} keep the three-state RHS off the heap, reducing GC noise.
  \item Fixed-size math improves cache locality and throughput for tight ODE kernels.
  \item \texttt{@inbounds} removes bounds checks once dimensions are known at compile time.
  \item \texttt{@inline} encourages the compiler to fuse the RHS into the solver loop, trimming call overhead.
  \item Fewer checks and calls leave the CPU focused on arithmetic in the hot loop.
\end{itemize}

\medskip

\noindent
We optimize each implementation using macros \texttt{@inbounds}, \texttt{@inline}, and we see
the lowered code to understand the impact of these changes.


\subsection*{Results and Analysis}

% --- Figures: speedup plot ---
\noindent
\begin{minipage}{\linewidth}
\centering
\begin{minipage}[t]{0.475\linewidth} % Left figure
  \includegraphics[width=0.95\linewidth]{rk4_fixed_speedup_solve_log10.png}
  {\captionsetup{hypcap=false}%
  \captionof{figure}{Experiment 1 speedup for fixed-step RK4 solves relative to the naïve out-of-place baseline `rossler\_naive`, using $\text{speedup}=\mathrm{median\_time}(\texttt{rossler\_naive})/\mathrm{median\_time}(\text{variant})$.}}
\end{minipage}
\hfill
\begin{minipage}[t]{0.475\linewidth} % Right figure
  \includegraphics[width=0.95\linewidth]{rk4_fixed_allocs_solve.png}
  {\captionsetup{hypcap=false}%
  \captionof{figure}{Allocations per RK4 solve. In-place and \texttt{SVector} variants eliminate heap traffic, enabling predictable runtimes across long integrations.}}
\end{minipage}

\medskip
\end{minipage}

\begin{table}[ht]
\centering
\resizebox{0.8\linewidth}{!}{%
  \begin{tabular}{lrrrr}
    \toprule
    Variant & Median (ns) & Median (ms) & Allocs & Bytes \\
    \midrule
    naïve (alloc) & 42 & 4.2e-05 & 2 & 80 \\
    oop (tuned) & 42 & 4.2e-05 & 2 & 80 \\
    in-place (naïve) & 41 & 4.1e-05 & 0 & 0 \\
    in-place (tuned) & 20.5 & 2.05e-05 & 0 & 0 \\
    SVector (naïve) & 0.001 & 1e-09 & 0 & 0 \\
    SVector (tuned) & 0.001 & 1e-09 & 0 & 0 \\
    type-stable & 42 & 4.2e-05 & 2 & 80 \\
    AD-ready & 41 & 4.1e-05 & 0 & 0 \\
    \bottomrule
  \end{tabular}
}
\caption{Per-call Rössler RHS benchmark medians and allocations across implementation variants.}
\end{table}

\vspace{1em}

\begin{table}[ht]
\centering
\resizebox{0.8\linewidth}{!}{%
  \begin{tabular}{lrrrrrr}
    \toprule
    Variant & dt & nsteps & Median (ns) & Median (ms) & Allocs & Bytes \\
    \midrule
    naïve (alloc) & 0.01 & 20000 & 7.41e+06 & 7.41 & 640066 & 25603088 \\
    oop (tuned) & 0.01 & 20000 & 7.31e+06 & 7.31 & 640066 & 25603088 \\
    in-place (naïve) & 0.01 & 20000 & 2.03e+06 & 2.03 & 50 & 2624 \\
    in-place (tuned) & 0.01 & 20000 & 1.88e+06 & 1.88 & 50 & 2624 \\
    SVector (naïve) & 0.01 & 20000 & 1.64e+06 & 1.64 & 22 & 1504 \\
    SVector (tuned) & 0.01 & 20000 & 1.46e+06 & 1.46 & 22 & 1504 \\
    type-stable & 0.01 & 20000 & 8.27e+06 & 8.27 & 640066 & 25603088 \\
    AD-ready & 0.01 & 20000 & 1.55e+06 & 1.55 & 22 & 1504 \\
    \bottomrule
  \end{tabular}
}
\caption{Fixed-step RK4 solve benchmarks for $t \in [0,T]$ with step size $h$; allocations reported per solve.}
\end{table}

\medskip

\noindent\textbf{Key observations.}
\begin{itemize}
  \item Tuned in-place RHS cuts per-call time roughly in half versus naïve out-of-place, and the \texttt{SVector} versions avoid heap allocation and are effectively allocation-free.
  \item Solve-level allocations drop from $\approx6.4\times 10^5$ to a few dozen (or zero) once the RHS stops allocating, stabilizing runtimes across long horizons.
  \item Fastest full solves come from the tuned \texttt{SVector} variant ($\sim5\times$ faster than baseline), closely followed by the AD-ready implementation that keeps allocations negligible.
\end{itemize}

\medskip
\hrulefill
\medskip

\switchcolumn
%----------------------------------------------------------------------------------------
% BEGIN: THIRD COLUMN
%----------------------------------------------------------------------------------------
%------------------------ Conclusion ------------------------
\section*{Performance ladder for Rössler RHS variants (L0–L5).}

\medskip
\hrulefill
\medskip

\begin{minipage}[c]{0.45\linewidth}
  \centering
  \includegraphics[width=\linewidth]{optimizations.png}
\end{minipage}

\medskip
\hrulefill 
\medskip 

%------------------------ Conclusion ------------------------
\section*{Conclusion: Optimizing ODE systems in Julia}
\noindent
Experiment 1 confirms that tightening the RHS kernel delivers most of the speedup: moving from naïve allocation to tuned in-place or \texttt{SVector} code removes GC noise, shrinks solve time by $\sim5\times$, and keeps the solver stable across long trajectories.

\medskip


\textbf{Practical tips for fast RHS code.}
\begin{itemize}
  \item Write in-place updates for solver-friendly mutability; reserve allocations for setup, not per-step work.
  \item Use \texttt{SVector} for very small systems to keep data off the heap; fall back to \texttt{Vector} for larger dimensions to avoid compile-time bloat.
  \item Validate correctness, then add \texttt{@inbounds} and \texttt{@inline} to strip bounds checks and call overhead; keep arguments type-stable so the compiler can inline aggressively.
\end{itemize}


\medskip

\noindent\textbf{Outlook.} The same kernel work carries to stiff problems and differentiation workflows; adding implicit integrators or adjoints next should reuse these allocation-free RHS variants without sacrificing performance.


\noindent
%------------------------ REFERENCES ------------------------
\nocite{*}
\bibliographystyle{plain}
\bibliography{mybib}

\end{paracol}
\end{document}
